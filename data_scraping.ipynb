{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCsconvQaJTF",
        "outputId": "7c6ce8d6-2c71-47b3-ff08-bec5a3375863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mounting colab with drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n"
      ],
      "metadata": {
        "id": "hcFlQYDTaMvR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting Flats/Apartments\n"
      ],
      "metadata": {
        "id": "RmbhgWuE-NXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "- Your_Project_Directory\n",
        "  - Data\n",
        "    - City\n",
        "      - Flats\n",
        "      - Societies\n",
        "      - Residential\n",
        "      - Independent House\n",
        "```"
      ],
      "metadata": {
        "id": "FKiPwsriwwd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to change as per your requirement - city name\n",
        "# Taking value of city as 'chandigarh'\n",
        "City = 'Mumbai'"
      ],
      "metadata": {
        "id": "rR9tZMhFxqy9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# User Agent\n",
        "# Headers set like below:\n",
        "# User Agent\n",
        "headers = {\n",
        "    'authority': 'www.mobilebricks.com',\n",
        "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
        "    'accept-language': 'en-US,en;q=0.9',\n",
        "    'cache-control': 'no-cache',\n",
        "    'dnt': '1',\n",
        "    'pragma': 'no-cache',\n",
        "    'referer': f'https://www.mobilebricks.com/flats-in-{City}-ffid-page',\n",
        "    'sec-ch-ua': '\"Chromium\";v=\"107\", \"Not;A=Brand\";v=\"8\"',\n",
        "    'sec-ch-ua-mobile': '?0',\n",
        "    'sec-ch-ua-platform': '\"macOS\"',\n",
        "    'sec-fetch-dest': 'document',\n",
        "    'sec-fetch-mode': 'navigate',\n",
        "    'sec-fetch-site': 'same-origin',\n",
        "    'sec-fetch-user': '?1',\n",
        "    'upgrade-insecure-requests': '1',\n",
        "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/527.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36',\n",
        "}"
      ],
      "metadata": {
        "id": "azjm7TTtGRl9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If folder structures are in already created no need to run it.\n",
        "\n",
        "import os\n",
        "\n",
        "# Define the path to your project directory\n",
        "project_dir = '/content/drive/MyDrive/pro/Real estate/'\n",
        "\n",
        "# Define the subdirectories\n",
        "subdirectories = ['Data', f'Data/{City}', f'Data/{City}/Flats', f'Data/{City}/Societies', f'Data/{City}/Residential', f'Data/{City}/Independent House']\n",
        "\n",
        "# Create the directory structure\n",
        "for subdir in subdirectories:\n",
        "    dir_path = os.path.join(project_dir, subdir)\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "        print(f\"Created directory: {dir_path}\")\n",
        "    else:\n",
        "        print(f\"Directory already exists: {dir_path}\")\n",
        "\n",
        "# Now, your directory structure is created.\n"
      ],
      "metadata": {
        "id": "WACPewbX0mz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edc167fb-e770-47b9-e8ac-971961f5316a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory already exists: /content/drive/MyDrive/pro/Real estate/Data\n",
            "Directory already exists: /content/drive/MyDrive/pro/Real estate/Data/Mumbai\n",
            "Directory already exists: /content/drive/MyDrive/pro/Real estate/Data/Mumbai/Flats\n",
            "Directory already exists: /content/drive/MyDrive/pro/Real estate/Data/Mumbai/Societies\n",
            "Directory already exists: /content/drive/MyDrive/pro/Real estate/Data/Mumbai/Residential\n",
            "Directory already exists: /content/drive/MyDrive/pro/Real estate/Data/Mumbai/Independent House\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json"
      ],
      "metadata": {
        "id": "AIhXlgb2jtN_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://www.magicbricks.com/flats-in-mumbai-for-sale-pppfs/page-1'\n",
        "page = requests.get(url, headers=headers)\n",
        "pageSoup = BeautifulSoup(page.content, 'html.parser')"
      ],
      "metadata": {
        "id": "DY-gMzZaYmtr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for soup in pageSoup.select_one('div.mb-srp__left').select_one('div.mb-srp__list'):\n",
        "    print(soup.select_one('h2.mb-srp__card--title').text.strip())\n",
        "    # Extract the text content from the script tags\n",
        "    script_texts = [script.text for script in soup.select('script')]\n",
        "    # Search for the pattern in the extracted text content\n",
        "    match = re.search(r'\\{.*\\}', ''.join(script_texts))\n",
        "    if match:\n",
        "        print(type(match.group()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kB1Sqi73lnmH",
        "outputId": "90aeee5c-877d-47c4-c385-71ed38a92cc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 BHK Flat for Sale in Goregaon West, Mumbai\n",
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for soup in pageSoup.select_one('div.mb-srp__left').select_one('div.mb-srp__list'):\n",
        "  json_data = soup.find('script', type='application/ld+json').string.strip()\n",
        "  # Parse the JSON data\n",
        "  try:\n",
        "    data = json.loads(json_data)\n",
        "\n",
        "    print(\"Name:\", data.get('name', 'N/A'))\n",
        "    print(\"Type:\", data.get('@type', 'N/A'))\n",
        "    print(\"Seller Name:\", data.get('potentialAction', {}).get('seller', {}).get('name', 'N/A'))\n",
        "    print(\"Latitude:\", data.get('geo', {}).get('latitude', 'N/A'))\n",
        "    print(\"Longitude:\", data.get('geo', {}).get('longitude', 'N/A'))\n",
        "    print(\"Locality:\", data.get('address', {}).get('addressLocality', 'N/A'))\n",
        "    print(\"Region:\", data.get('address', {}).get('addressRegion', 'N/A'))\n",
        "    print(\"Country:\", data.get('address', {}).get('addressCountry', 'N/A'))\n",
        "    url_in = data.get('url', 'N/A')\n",
        "    if url_in != 'N/A':\n",
        "      page_in = requests.get(url_in, headers=headers)\n",
        "      pageSoup_in = BeautifulSoup(page_in.content, 'html.parser')\n",
        "\n",
        "      div = pageSoup_in.find('div', class_='mb-ldp__dtls__body__list')\n",
        "      main_text = f\"{div.contents[0].strip()} {div.find('span', class_='mb-ldp__dtls__body__list--units').find('span').text}\"\n",
        "\n",
        "      summary = pageSoup_in.find_all('li', class_='mb-ldp__dtls__body__summary--item')\n",
        "\n",
        "      summary_text = [su.get_text(strip=True) for su in summary]\n",
        "      result = ', '.join(summary_text)\n",
        "\n",
        "      print(result)\n",
        "      items = pageSoup_in.find_all('div', class_='mb-ldp__dtls__body__summary--item isPremium')\n",
        "\n",
        "      # Extract the text from each div and join them into a comma-separated string\n",
        "      text_list = [item.get_text(strip=True) for item in items]\n",
        "      result = ', '.join(text_list)\n",
        "\n",
        "      print(result)\n",
        "      print(pageSoup_in.find('div',class_='mb-ldp__dtls__price').text.strip())\n",
        "\n",
        "      amenities = pageSoup_in.find_all('li', class_='mb-ldp__amenities__list--item')\n",
        "\n",
        "      amenities_text = [amenity.get_text(strip=True) for amenity in amenities]\n",
        "      result = ', '.join(amenities_text)\n",
        "\n",
        "      print(result)\n",
        "      data_dict = {}\n",
        "      for item in pageSoup_in.find_all('li',class_='mb-ldp__dtls__body__list--item'):\n",
        "        label = item.find('div', class_='mb-ldp__dtls__body__list--label').get_text(strip=True)\n",
        "        value_div = item.find('div', class_='mb-ldp__dtls__body__list--value')\n",
        "\n",
        "        link = value_div.find('a')\n",
        "        if link:\n",
        "            value = link.get_text(strip=True)\n",
        "        else:\n",
        "            value = value_div.get_text(strip=True)\n",
        "\n",
        "        if label == 'Carpet Area' or label == 'Super Built-up Area':\n",
        "          size_text = item.find('div', class_='mb-ldp__dtls__body__list--size').get_text(strip=True)\n",
        "          value = main_text + ',' + size_text\n",
        "        data_dict[label] = value\n",
        "      print(data_dict)\n",
        "\n",
        "  except json.JSONDecodeError as e:\n",
        "    print(f\"Error parsing JSON: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "FKUkma2N764l",
        "outputId": "c4aee3db-c744-44f1-d64b-0a2224db2c2f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'json' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2daff62e8772>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2daff62e8772>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error parsing JSON: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for soup in pageSoup.select_one('div.mb-srp__left').select_one('div.mb-srp__list'):\n",
        "  json_data = soup.find('script', type='application/ld+json').string.strip()\n",
        "  # Parse the JSON data\n",
        "  try:\n",
        "    data = json.loads(json_data)\n",
        "\n",
        "    print(\"Name:\", data.get('name', 'N/A'))\n",
        "    print(\"Type:\", data.get('@type', 'N/A'))\n",
        "    print(\"Seller Name:\", data.get('potentialAction', {}).get('seller', {}).get('name', 'N/A'))\n",
        "    print(\"Latitude:\", data.get('geo', {}).get('latitude', 'N/A'))\n",
        "    print(\"Longitude:\", data.get('geo', {}).get('longitude', 'N/A'))\n",
        "    print(\"Locality:\", data.get('address', {}).get('addressLocality', 'N/A'))\n",
        "    print(\"Region:\", data.get('address', {}).get('addressRegion', 'N/A'))\n",
        "    print(\"Country:\", data.get('address', {}).get('addressCountry', 'N/A'))\n",
        "    url_in = data.get('url', 'N/A')\n",
        "    if url_in != 'N/A':\n",
        "      page_in = requests.get(url_in, headers=headers)\n",
        "      pageSoup_in = BeautifulSoup(page_in.content, 'html.parser')\n",
        "      # print(pageSoup_in.find('div',class_='mb-ldp__dtls__body__list').text.strip())\n",
        "      # Find the div with the specified class\n",
        "      div = pageSoup_in.find('div', class_='mb-ldp__dtls__body__list')\n",
        "\n",
        "      # # Extract the main text (1072)\n",
        "      # main_text = div.contents[0].strip()\n",
        "\n",
        "      # # Extract the unit (sqft)\n",
        "      # unit_text = div.find('span', class_='mb-ldp__dtls__body__list--units').find('span').text\n",
        "\n",
        "      # Combine the main text and unit text\n",
        "      main_text = f\"{div.contents[0].strip()} {div.find('span', class_='mb-ldp__dtls__body__list--units').find('span').text}\"\n",
        "\n",
        "      # Print the result\n",
        "      # print(result)\n",
        "      print(pageSoup_in.find('ul',class_='mb-ldp__dtls__body__summary').text.strip())\n",
        "      # print(pageSoup_in.find('div',class_='mb-ldp__dtls__body__summary--item ico-beds').text.strip())\n",
        "      # print(pageSoup_in.find('div',class_='mb-ldp__dtls__body__summary--item ico-baths').text.strip())\n",
        "      # print(pageSoup_in.find('div',class_='mb-ldp__dtls__body__summary--item ico-balconies').text.strip())\n",
        "      # print(pageSoup_in.find('div',class_='mb-ldp__dtls__body__summary--item ico-furnished').text.strip())\n",
        "      # print(pageSoup_in.find('div',class_='mb-ldp__dtls__body__summary--right').text.strip())\n",
        "      # Find all divs with class \"mb-ldp__dtls__body__summary--item isPremium\"\n",
        "      items = pageSoup_in.find_all('div', class_='mb-ldp__dtls__body__summary--item isPremium')\n",
        "\n",
        "      # Extract the text from each div and join them into a comma-separated string\n",
        "      text_list = [item.get_text(strip=True) for item in items]\n",
        "      result = ', '.join(text_list)\n",
        "\n",
        "      # Print the result\n",
        "      print(result)\n",
        "      print(pageSoup_in.find('div',class_='mb-ldp__dtls__price').text.strip())\n",
        "      # print(pageSoup_in.find('ul',{'class':'mb-ldp__lux'}).text.strip())\n",
        "      # Find all span elements with class \"amenities__item__text\"\n",
        "      amenities = pageSoup_in.find_all('li', class_='mb-ldp__amenities__list--item')\n",
        "\n",
        "      # Extract the text from each span and join them into a comma-separated string\n",
        "      amenities_text = [amenity.get_text(strip=True) for amenity in amenities]\n",
        "      result = ', '.join(amenities_text)\n",
        "\n",
        "      # Print the result\n",
        "      print(result)\n",
        "      data_dict = {}\n",
        "      for item in pageSoup_in.find_all('li',class_='mb-ldp__dtls__body__list--item'):\n",
        "        label = item.find('div', class_='mb-ldp__dtls__body__list--label').get_text(strip=True)\n",
        "        value_div = item.find('div', class_='mb-ldp__dtls__body__list--value')\n",
        "\n",
        "        # If there's a link in the value\n",
        "        link = value_div.find('a')\n",
        "        if link:\n",
        "            value = link.get_text(strip=True)\n",
        "        else:\n",
        "            value = value_div.get_text(strip=True)\n",
        "\n",
        "        if label == 'Carpet Area' or label == 'Super Built-up Area':\n",
        "          # main_text = item.find('div', class_='mb-ldp__dtls__body__list').get_text(strip=True)\n",
        "          size_text = item.find('div', class_='mb-ldp__dtls__body__list--size').get_text(strip=True)\n",
        "          value = main_text + ',' + size_text\n",
        "        data_dict[label] = value\n",
        "      # item = pageSoup_in.find('li', class_='mb-ldp__dtls__body__list--item')\n",
        "      # data_dict['Super Built-up Area'] = item.find('div', class_='mb-ldp__dtls__body__list').get_text(strip=True) + item.find('div', class_='mb-ldp__dtls__body__list--size').get_text(strip=True)\n",
        "      print(data_dict)\n",
        "\n",
        "  except json.JSONDecodeError as e:\n",
        "    print(f\"Error parsing JSON: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJXlRPdDpvfA",
        "outputId": "b044acdf-152c-4a47-ce6b-009f0a5bdde2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: 2 BHK Flat  for Sale in  NG Royal Park, Kanjurmarg East, Mumbai\n",
            "Type: Apartment\n",
            "Seller Name: kanayalal\n",
            "Latitude: 19.12021420298\n",
            "Longitude: 72.9299633704782\n",
            "Locality: Kanjurmarg East\n",
            "Region: Mumbai\n",
            "Country: IN\n",
            "2Beds2BathsSemi-Furnished\n",
            "\n",
            "₹1.50 Cr\n",
            "Power Back Up, Lift, Club House, Swimming Pool, Gymnasium, Park\n",
            "{'Carpet Area': '625 sqft,₹24,000/sqft', 'Developer': 'RNA NG Builders', 'Project': 'NG Royal Park', 'Floor': '5 (Out of 15 Floors)', 'Transaction type': 'Resale', 'Status': 'Ready to Move', 'Additional Rooms': '1 Store Room', 'Facing': 'East', 'Lifts': '3'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url_in"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aw3UAd2I5geD",
        "outputId": "f727f5a8-b45b-423b-9535-64d98afdf252"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://www.magicbricks.com/propertyDetails/2-BHK-953-Sq-ft-Multistorey-Apartment-FOR-Sale-Kanjurmarg-East-in-Mumbai&id=4d423731313133363135'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pageSoup_in"
      ],
      "metadata": {
        "id": "5F1dhXl3q-f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for soup in pageSoup.select_one('div.mb-srp__left').select_one('div.mb-srp__list'):\n",
        "#   json_data = soup.find('script', type='application/ld+json').string.strip()\n",
        "#   # Parse the JSON data\n",
        "#   try:\n",
        "#     data = json.loads(json_data)\n",
        "\n",
        "#     # Access and print all fields in the JSON data\n",
        "#     # print(\"Context:\", data.get('@context', 'N/A'))\n",
        "#     print(\"Type:\", data.get('@type', 'N/A'))\n",
        "#     # print(\"ID:\", data.get('@id', 'N/A'))\n",
        "#     print(\"URL:\", data.get('url', 'N/A'))\n",
        "#     print(\"Number of Rooms:\", data.get('numberOfRooms', 'N/A'))\n",
        "#     print(\"Image URL:\", data.get('image', 'N/A'))\n",
        "#     print(\"Name:\", data.get('name', 'N/A'))\n",
        "#     # print(\"Geo Type:\", data.get('geo', {}).get('@type', 'N/A'))\n",
        "#     print(\"Latitude:\", data.get('geo', {}).get('latitude', 'N/A'))\n",
        "#     print(\"Longitude:\", data.get('geo', {}).get('longitude', 'N/A'))\n",
        "#     # print(\"Action Type:\", data.get('potentialAction', {}).get('@type', 'N/A'))\n",
        "#     # print(\"Seller Type:\", data.get('potentialAction', {}).get('seller', {}).get('@type', 'N/A'))\n",
        "#     print(\"Seller Name:\", data.get('potentialAction', {}).get('seller', {}).get('name', 'N/A'))\n",
        "#     # print(\"Address Type:\", data.get('address', {}).get('@type', 'N/A'))\n",
        "#     print(\"Locality:\", data.get('address', {}).get('addressLocality', 'N/A'))\n",
        "#     print(\"Region:\", data.get('address', {}).get('addressRegion', 'N/A'))\n",
        "#     print(\"Country:\", data.get('address', {}).get('addressCountry', 'N/A'))\n",
        "\n",
        "#   except json.JSONDecodeError as e:\n",
        "#     print(f\"Error parsing JSON: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoD4Nl-YnBh3",
        "outputId": "ad47ba9d-5f6f-4032-a092-7565e3ab4322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type: Apartment\n",
            "URL: https://www.magicbricks.com/propertyDetails/3-BHK-1072-Sq-ft-Multistorey-Apartment-FOR-Sale-Goregaon-West-in-Mumbai&id=4d423731393639323937\n",
            "Number of Rooms: 3\n",
            "Image URL: https://img.staticmb.com/mbphoto/property/cropped_images/2024/Apr/30/Photo_h180_w240/71969297_1_PropertyImage670-5736003633787_180_240.jpg\n",
            "Name: 3 BHK Flat  for Sale in  Wadhwa Anmol Tower, Goregaon West, Mumbai\n",
            "Latitude: 19.1703700678522\n",
            "Longitude: 72.8424722723454\n",
            "Seller Name: Krunal Pandya\n",
            "Locality: Goregaon West\n",
            "Region: Mumbai\n",
            "Country: IN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "city = 'mumbai'\n",
        "for i in range(2):\n",
        "  url = f'https://www.magicbricks.com/flats-in-{city}-for-sale-pppfs/page-{i}'\n",
        "  page = requests.get(url, headers=headers)\n",
        "  pageSoup = BeautifulSoup(page.content, 'html.parser')\n",
        "  for soup in pageSoup.select_one('div.mb-srp__left').select_one('div.mb-srp__list'):\n",
        "    json_data = soup.find('script', type='application/ld+json').string.strip()\n",
        "    # Parse the JSON data\n",
        "    try:\n",
        "      data = json.loads(json_data)\n",
        "\n",
        "      print(\"Name:\", data.get('name', 'N/A'))\n",
        "      print(\"Type:\", data.get('@type', 'N/A'))\n",
        "      print(\"Seller Name:\", data.get('potentialAction', {}).get('seller', {}).get('name', 'N/A'))\n",
        "      print(\"Latitude:\", data.get('geo', {}).get('latitude', 'N/A'))\n",
        "      print(\"Longitude:\", data.get('geo', {}).get('longitude', 'N/A'))\n",
        "      print(\"Locality:\", data.get('address', {}).get('addressLocality', 'N/A'))\n",
        "      print(\"Region:\", data.get('address', {}).get('addressRegion', 'N/A'))\n",
        "      print(\"Country:\", data.get('address', {}).get('addressCountry', 'N/A'))\n",
        "      url_in = data.get('url', 'N/A')\n",
        "      if url_in != 'N/A':\n",
        "        page_in = requests.get(url_in, headers=headers)\n",
        "        pageSoup_in = BeautifulSoup(page_in.content, 'html.parser')\n",
        "\n",
        "        summary = pageSoup_in.find_all('li', class_='mb-ldp__dtls__body__summary--item')\n",
        "\n",
        "        summary_text = [su.get_text(strip=True) for su in summary]\n",
        "        result = ', '.join(summary_text)\n",
        "\n",
        "        print(result)\n",
        "        items = pageSoup_in.find_all('div', class_='mb-ldp__dtls__body__summary--item isPremium')\n",
        "\n",
        "        # Extract the text from each div and join them into a comma-separated string\n",
        "        text_list = [item.get_text(strip=True) for item in items]\n",
        "        result = ', '.join(text_list)\n",
        "\n",
        "        print(result)\n",
        "        print(pageSoup_in.find('div',class_='mb-ldp__dtls__price').text.strip())\n",
        "\n",
        "        amenities = pageSoup_in.find_all('li', class_='mb-ldp__amenities__list--item')\n",
        "\n",
        "        amenities_text = [amenity.get_text(strip=True) for amenity in amenities]\n",
        "        result = ', '.join(amenities_text)\n",
        "\n",
        "        print(result)\n",
        "        data_dict = {}\n",
        "        div = pageSoup_in.find('div', class_='mb-ldp__dtls__body__list')\n",
        "        main_text = f\"{div.contents[0].strip()} {div.find('span', class_='mb-ldp__dtls__body__list--units').find('span').text}\"\n",
        "        for item in pageSoup_in.find_all('li',class_='mb-ldp__dtls__body__list--item'):\n",
        "          label = item.find('div', class_='mb-ldp__dtls__body__list--label').get_text(strip=True)\n",
        "          value_div = item.find('div', class_='mb-ldp__dtls__body__list--value')\n",
        "\n",
        "          link = value_div.find('a')\n",
        "          if link:\n",
        "              value = link.get_text(strip=True)\n",
        "          else:\n",
        "              value = value_div.get_text(strip=True)\n",
        "\n",
        "          if label == 'Carpet Area' or label == 'Super Built-up Area':\n",
        "            size_text = item.find('div', class_='mb-ldp__dtls__body__list--size').get_text(strip=True)\n",
        "            value = main_text + ',' + size_text\n",
        "          data_dict[label] = value\n",
        "        print(data_dict)\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "      print(f\"Error parsing JSON: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4Oa-w4qAZzY",
        "outputId": "9460d581-8a30-489b-e46a-2873c8aaaa3f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: 2 BHK Flat  for Sale in  Ghatkopar West, Mumbai\n",
            "Type: Apartment\n",
            "Seller Name: Ayush Shah\n",
            "Latitude: 19.0908063\n",
            "Longitude: 72.9076669\n",
            "Locality: Ghatkopar West\n",
            "Region: Mumbai\n",
            "Country: IN\n",
            "2Beds, 2Baths, Furnished\n",
            "\n",
            "₹1.20 Cr\n",
            "\n",
            "{'Super Built-up Area': '754 sqft,₹15,915/sqft', 'Floor': '2 (Out of 5 Floors)', 'Transaction type': 'Resale', 'Status': 'Ready to Move', 'Furnished Status': 'Furnished'}\n",
            "Name: 2 BHK Flat  for Sale in  NG Royal Park, Kanjurmarg East, Mumbai\n",
            "Type: Apartment\n",
            "Seller Name: kanayalal\n",
            "Latitude: 19.12021420298\n",
            "Longitude: 72.9299633704782\n",
            "Locality: Kanjurmarg East\n",
            "Region: Mumbai\n",
            "Country: IN\n",
            "2Beds, 2Baths, Semi-Furnished\n",
            "\n",
            "₹1.50 Cr\n",
            "Power Back Up, Lift, Club House, Swimming Pool, Gymnasium, Park\n",
            "{'Carpet Area': '625 sqft,₹24,000/sqft', 'Developer': 'RNA NG Builders', 'Project': 'NG Royal Park', 'Floor': '5 (Out of 15 Floors)', 'Transaction type': 'Resale', 'Status': 'Ready to Move', 'Additional Rooms': '1 Store Room', 'Facing': 'East', 'Lifts': '3'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "headers={'User-Agent':'Mozilla/5.0 (Windows NT 6.3; Win 64 ; x64) Apple WeKit /537.36(KHTML , like Gecko) Chrome/80.0.3987.162 Safari/537.36'}"
      ],
      "metadata": {
        "id": "ENGR2hM3LnJM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Put start page number and end page number.\n",
        "\n",
        "# Page number to start extraction data\n",
        "start = int(input(\"Enter page number where you got error in last run.\\nEnter page number to start from:\")) # Starting Page\n",
        "\n",
        "# End Page number- you can change is for start i am taking 10pages at a time,\n",
        "# as IPs are gettig block after some time\n",
        "end = start+3\n",
        "\n",
        "pageNumber = start\n",
        "req=0\n",
        "\n",
        "flats = pd.DataFrame()\n",
        "\n",
        "try :\n",
        "    while pageNumber < end:\n",
        "        i=1\n",
        "        url = 'https://www.magicbricks.com/flats-in-{City}-for-sale-pppfs/page-{pageNumber}'.format(City=city,pageNumber=pageNumber)\n",
        "        page = requests.get(url, headers=headers)\n",
        "        pageSoup = BeautifulSoup(page.content, 'html.parser')\n",
        "        req+=1\n",
        "        for soup in pageSoup.select_one('div.mb-srp__left').select_one('div.mb-srp__list'):\n",
        "            json_data = soup.find('script', type='application/ld+json').string.strip()\n",
        "            # Extract property name and property sub-name\n",
        "            try:\n",
        "                data = json.loads(json_data)\n",
        "\n",
        "                property_name = data.get('name', 'N/A')\n",
        "                seller_name = data.get('potentialAction', {}).get('seller', {}).get('name', 'N/A')\n",
        "                latitude = data.get('geo', {}).get('latitude', 'N/A')\n",
        "                longitude = data.get('geo', {}).get('longitude', 'N/A')\n",
        "                region = data.get('address', {}).get('addressRegion', 'N/A')\n",
        "                country = data.get('address', {}).get('addressCountry', 'N/A')\n",
        "                # Area\n",
        "                try:\n",
        "                    area = data.get('address', {}).get('addressLocality', 'N/A')\n",
        "                except:\n",
        "                    area =''\n",
        "                # Type\n",
        "                try:\n",
        "                    Type = data.get('@type', 'N/A')\n",
        "                except:\n",
        "                    Type = ''\n",
        "                # Extract link\n",
        "                url_in = data.get('url', 'N/A')\n",
        "            except:\n",
        "                continue\n",
        "            # Detail Page\n",
        "            if url_in != 'N/A':\n",
        "              page_in = requests.get(url_in, headers=headers)\n",
        "              pageSoup_in = BeautifulSoup(page_in.content, 'html.parser')\n",
        "            req += 1\n",
        "            try:\n",
        "                #price Range\n",
        "                price = pageSoup_in.find('div',class_='mb-ldp__dtls__price').text.strip()\n",
        "            except:\n",
        "                price = ''\n",
        "\n",
        "            # Configuration\n",
        "            try:\n",
        "                summary = pageSoup_in.find_all('li', class_='mb-ldp__dtls__body__summary--item')\n",
        "                if len(summary) == 0:\n",
        "                    summary = pageSoup_in.find('div',class_='mb-ldp__dtls__body__summary--left mb-ldp__dtls__body__summary--dflex').find_all('div', class_='mb-ldp__dtls__body__summary--item')\n",
        "\n",
        "                summary_text = [su.get_text(strip=True) for su in summary]\n",
        "                config = ', '.join(summary_text)\n",
        "            except:\n",
        "                config = ''\n",
        "\n",
        "\n",
        "            # Premium Items\n",
        "\n",
        "            try:\n",
        "                items = pageSoup_in.find_all('div', class_='mb-ldp__dtls__body__summary--item isPremium')\n",
        "\n",
        "                # Extract the text from each div and join them into a comma-separated string\n",
        "                text_list = [item.get_text(strip=True) for item in items]\n",
        "                premium = ', '.join(text_list)\n",
        "            except:\n",
        "                premium = ''\n",
        "            # Amenities\n",
        "            try:\n",
        "                amenities = pageSoup_in.find_all('li', class_='mb-ldp__amenities__list--item')\n",
        "\n",
        "                amenities_text = [amenity.get_text(strip=True) for amenity in amenities]\n",
        "                amenit = ', '.join(amenities_text)\n",
        "            except:\n",
        "                amenit = ''\n",
        "\n",
        "            data_dict = {}\n",
        "            div = pageSoup_in.find('div', class_='mb-ldp__dtls__body__list')\n",
        "            main_text = f\"{div.contents[0].strip()} {div.find('span', class_='mb-ldp__dtls__body__list--units').find('span').text}\"\n",
        "            for item in pageSoup_in.find_all('li',class_='mb-ldp__dtls__body__list--item'):\n",
        "              label = item.find('div', class_='mb-ldp__dtls__body__list--label').get_text(strip=True)\n",
        "              value_div = item.find('div', class_='mb-ldp__dtls__body__list--value')\n",
        "\n",
        "              link = value_div.find('a')\n",
        "              if link:\n",
        "                  value = link.get_text(strip=True)\n",
        "              else:\n",
        "                  value = value_div.get_text(strip=True)\n",
        "\n",
        "              if label == 'Carpet Area' or label == 'Super Built-up Area':\n",
        "                size_text = item.find('div', class_='mb-ldp__dtls__body__list--size').get_text(strip=True)\n",
        "                value = main_text + ',' + size_text\n",
        "              data_dict[label] = value\n",
        "\n",
        "            # Create a dictionary with the given variables\n",
        "            property_data = {\n",
        "            'property_name': property_name,\n",
        "            'link': url_in,\n",
        "            'seller_name': seller_name,\n",
        "            'latitude': latitude,\n",
        "            'longitude': longitude,\n",
        "            'region': region,\n",
        "            'country': country,\n",
        "            'config': config,\n",
        "            'premium': premium,\n",
        "            'amenities': amenit,\n",
        "            'price': price,\n",
        "            'area': area,\n",
        "            'Type': Type,\n",
        "            'data_dict': data_dict\n",
        "        }\n",
        "\n",
        "\n",
        "            temp_df = pd.DataFrame.from_records([property_data])\n",
        "            # print(temp_df)\n",
        "            flats = pd.concat([flats, temp_df], ignore_index=True)\n",
        "            i += 1\n",
        "            # if os.path.isfile(csv_file):\n",
        "            # # Append DataFrame to the existing file without header\n",
        "            #     temp_df.to_csv(csv_file, mode='a', header=False, index=False)\n",
        "            # else:\n",
        "            #     # Write DataFrame to the file with header\n",
        "            #     temp_df.to_csv(csv_file, mode='a', header=True, index=False)\n",
        "\n",
        "            if req % 4==0:\n",
        "                time.sleep(10)\n",
        "            if req % 15 == 0:\n",
        "                time.sleep(50)\n",
        "        print(f'{pageNumber} -> {i}')\n",
        "        pageNumber += 1\n",
        "\n",
        "except AttributeError as e:\n",
        "    print(e)\n",
        "    print(\"----------------\")\n",
        "    print(\"\"\"Your IP might have blocked. Delete Runitme and reconnect again with updating start page number.\\n\n",
        "            You would see in output above like 1 -> 15\\ and so 1 is page number and 15 is data items extracted.\"\"\")\n",
        "csv_file_path = f\"/content/drive/MyDrive/pro/Real estate/Data/bhilai/Flats/flats_{City}_data-page-{start}-{pageNumber-1}.csv\"\n",
        "\n",
        "# This file will be new every time if start page will chnage, but still taking here mode as append\n",
        "if os.path.isfile(csv_file_path):\n",
        "# Append DataFrame to the existing file without header\n",
        "  flats.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
        "else:\n",
        "# Write DataFrame to the file with header - first time write\n",
        "  flats.to_csv(csv_file_path, mode='a', header=True, index=False)\n"
      ],
      "metadata": {
        "id": "QT0wWB0ISqy4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc3ed5a9-2e53-43ea-b384-d507a8d5214d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter page number where you got error in last run.\n",
            "Enter page number to start from:1\n",
            "'NoneType' object has no attribute 'select_one'\n",
            "----------------\n",
            "Your IP might have blocked. Delete Runitme and reconnect again with updating start page number.\n",
            "\n",
            "            You would see in output above like 1 -> 15\\ and so 1 is page number and 15 is data items extracted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### If getting errors"
      ],
      "metadata": {
        "id": "x4EP9dk20v1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Solution for colab\n",
        "```\n",
        "Go to menu bar:\n",
        "Runtime -> Disconnect and Delete runtime -> Reconnect again.\n",
        "```"
      ],
      "metadata": {
        "id": "1TgQVrJ33bfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to combine multiple csv file is one file.\n",
        "\n",
        "def combine_csv_files(folder_path, combined_file_path):\n",
        "    combined_data = pd.DataFrame()  # Create an empty DataFrame to hold the combined data\n",
        "\n",
        "    # Iterate through all CSV files in the folder\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.endswith('.csv'):\n",
        "            file_path = os.path.join(folder_path, file_name)\n",
        "            print('file_path')\n",
        "            # Read the data from the current CSV file\n",
        "            df = pd.read_csv(file_path)\n",
        "\n",
        "            # Append the data to the combined DataFrame\n",
        "            combined_data = combined_data.append(df, ignore_index=True)\n",
        "\n",
        "            # Delete the original CSV file\n",
        "            os.remove(file_path)\n",
        "\n",
        "    # Save the combined data to a new CSV file\n",
        "    combined_data.to_csv(combined_file_path, index=False)\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# Replace with the actual folder path\n",
        "folder_path = '/content/drive/MyDrive/DSMP/Case Studies/Real estate/flats_appartment'\n",
        "\n",
        "# Replace with the desired combined file path\n",
        "combined_file_path = '/content/drive/MyDrive/DSMP/Case Studies/Real estate/flats_appartment/flats.csv'\n",
        "\n",
        "combine_csv_files(folder_path, combined_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAVAsW5tgodt",
        "outputId": "990a3eb2-57aa-40a7-f3a3-d8222029b1ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file_path\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-26a7d1242761>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  combined_data = combined_data.append(df, ignore_index=True)\n",
            "<ipython-input-7-26a7d1242761>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  combined_data = combined_data.append(df, ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file_path\n"
          ]
        }
      ]
    }
  ]
}